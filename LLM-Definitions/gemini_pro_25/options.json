{
    "temperature": {
        "default": 1,
        "range": "0 - 2",
        "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
    },
    "top_k": {
        "default": 40,
        "range": "1 - 100",
        "description": "The topK parameter changes how the model selects tokens for output. A topK of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a topK of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the topK tokens with the highest probabilities are sampled. Tokens are then further filtered based on topP with the final token selected using temperature sampling."
    },
    "top_p": {
        "default": 0.95,
        "range": "0 - 1",
        "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
    },
    "max_tokens": {
        "default": 256,
        "range": "1 - 8192",
        "description": "Parameter to define how long the output can get."
    },
    "API_KEY": {
        "default": "Google",
        "range": "sk-****",
        "description": "This is the Google key that is used to make the actual request."
    }
}