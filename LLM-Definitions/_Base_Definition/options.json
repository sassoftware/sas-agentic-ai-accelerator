{
    "temperature": {
        "default": 1,
        "range": "0 - 2",
        "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
    },
    "top_p": {
        "default": 1,
        "range": "0 - 1",
        "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
    },
    "max_tokens": {
        "default": 1000,
        "range": "1 - 200000 (minus input tokent length)",
        "description": "Limits the amount of output tokens generated by the model."
    },
    "top_k": {
        "default": 40,
        "range": "1 - 100",
        "description": "The topK parameter changes how the model selects tokens for output. A topK of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a topK of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the topK tokens with the highest probabilities are sampled. Tokens are then further filtered based on topP with the final token selected using temperature sampling."
    },
    "API_KEY": {
        "default": "ProviderName",
        "range": "sk-****",
        "description": "This is the ProviderName key that is used to make the actual request."
    }
}